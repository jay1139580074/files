1. 服务边界划分困难
    原单体系统业务耦合严重，模块之间逻辑交叉，拆分时很难界定服务边界。
    我通过梳理业务流程、数据流向和调用链，结合 DDD（领域驱动设计）思想，逐步明确核心域与支撑域。
    引入 API 网关做统一入口，避免前端直接依赖后端服务，增强服务独立性。


    “在微服务拆分过程中，我没有直接按 Controller 或 DAO 拆模块，而是先从业务流程入手，梳理了用户行为路径和系统响应链。

    比如在scenario createion 系统中，我发现‘MD service load data’是高频且高价值流程，于是将其定义为核心域，
    重点设计领域模型和服务边界。
    
    而像‘客服处理’和‘报表生成’虽然必要，但不影响主流程，于是归为支撑域，采用轻量服务设计。
    通过这种方式，我们不仅提升了系统的弹性，也让团队在资源投入上更有策略。”

----------          ----------          ----------          ----------          ----------          

2. 数据一致性挑战                                           --  scenario creation
    拆分后各服务拥有独立数据库，原有的事务机制无法跨服务使用。

    我设计了基于事件驱动的异步通信机制（如使用 Kafka），实现最终一致性。

    对于关键业务场景，采用补偿机制和幂等性设计，确保数据准确性。
----------          ----------          ----------          ----------          ----------          

3. 老旧逻辑迁移与性能瓶颈                                       --  upload big file
    原系统存在大量嵌套查询和冗余逻辑，迁移后性能不达预期。

    我重构了核心查询逻辑，引入缓存策略（如 Redis）和分页优化，查询效率提升约 40%。

    使用 JMeter 做压测，定位瓶颈并逐步优化 SQL 和服务响应时间。
----------          ----------          ----------          ----------          ----------          

5. 团队协作与知识沉淀不足                                       --统一
    多人协作开发微服务，接口变更频繁，文档缺失导致沟通成本高。
    我推动统一 RESTful API 规范，并引入 Swagger 自动生成接口文档。  --前段推后端不给借口，Mock data
    编写 Runbook 和技术设计文档，确保新成员快速上手，减少知识流失。
----------          ----------          ----------          ----------          ----------    

6. 链路追踪与故障排查困难
    服务之间调用链复杂，出错时难以定位问题。

    我集成了 Spring Cloud Sleuth + Zipkin，实现调用链可视化。

    配合 ELK 日志系统，快速定位异常服务和请求路径。

----------          ----------          ----------          ----------          ----------    

微服务之间的通讯


实时请求响应	        RESTful API / gRPC
高性能内部调用	         gRPC
异步任务处理	        消息队列 / 事件驱动
服务治理与流量控制	     API 网关 / Service Mesh
多语言服务集成	         gRPC / REST
高并发削峰	            Kafka / RabbitMQ
----------          ----------          ----------          ----------          ---------- 

分表分库
在这个分库分表项目中，最大的技术挑战是跨库数据的一致性问题。由于业务逻辑涉及多个数据库的交互，传统的分布式事务机制（如 XA）无法满足性能和可扩展性的要求。为此，我主导设计了一套基于最终一致性原则的解决方案。

我们采用了事件驱动架构，将跨库操作拆分为多个独立事务。主库操作完成后，通过消息队列（如 Kafka）发送事件通知，由异步消费者在目标库执行补偿操作。为确保数据安全，我们设计了幂等机制，通过唯一事务ID避免重复消费导致的数据污染。同时，补偿逻辑支持失败重试，并配套事务日志表，便于追踪和人工干预。

此外，为解决主键冲突和数据可追踪性问题，我们统一了全局唯一ID生成策略，采用雪花算法结合业务标识，确保在多库环境下生成的ID既唯一又有序。


最大的挑战是跨库数据的一致性问题。由于业务逻辑涉及多个库的数据交互，传统事务机制无法满足需求。
我们设计了补偿机制，结合消息队列和重试策略，确保最终一致性。
同时在ID生成上也做了统一规划，避免了主键冲突。

由于无法使用分布式事务（如 XA），我们采用了补偿机制 + 异步消息队列来实现最终一致性：

1. 最终一致性方案设计
        ✅ 核心流程：
        业务操作拆分：将跨库操作拆分为多个独立事务，分别在各库中执行。

        事件驱动架构：主库操作成功后，发送事件消息到消息队列（如 Kafka / RocketMQ）。
                记录一条事务日志（包括操作类型、目标库、参数等）

        异步消费补偿：消费者监听事件并在目标库中执行补偿操作（如插入、更新等）。    --其他两个库异步消费并写入相同数据
                通过事务日志记录每个库的写入状态。   记录更新条数，多库对比
                

        幂等性保障：通过唯一事务ID或业务ID，确保补偿操作在重复消费时不会造成数据污染。64位ID生成算法，包含时间戳、机器ID、序列号

        失败重试机制：消费失败时自动重试，并记录失败日志供人工干预。
                1. 消费者设置手动offset，只有成功执行db 才会提交，
                2. 设置 factory.setCommonErrorHandler(errorHandler);--设置重试次数，超过次数会发死信队列
                3. 订阅死信，实现补偿机制       人工干预

》〉》〉》〉》〉》〉》〉》〉》〉》〉》〉》〉》〉》〉》〉》
        主库写入成功后，发送事件到消息队列。

        其他两个库异步消费并写入相同数据。

        通过事务日志记录每个库的写入状态。   记录更新条数，多库对比

2. 事务日志与补偿表设计
        每次主库操作成功后，记录一条事务日志（包括操作类型、目标库、参数等）。

        补偿服务定期扫描未完成的事务日志，进行补偿或重试。
                ！！

        支持人工干预和补偿状态追踪。
        
3. 全局唯一 ID 生成策略
        为避免主键冲突和数据追踪困难，设计了统一的 ID 生成机制：

        🧩 常见方案：
        雪花算法（Snowflake）：基于时间戳 + 机器ID + 序列号，生成64位唯一ID。

        数据库分段ID：每个库预分配一段ID区间，避免冲突。

        UUID + 库标识：使用 UUID 并附加库标识前缀，确保唯一性。        